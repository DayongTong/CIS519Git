{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  read txt file and convert to 1000 by 2 string 2D list\n",
    "\n",
    "file = open('badges/train/train.names.txt','r')\n",
    "lines = file.readlines()\n",
    "rows, cols = (len(lines), 2) \n",
    "txtarr = [[0]*cols]*rows \n",
    "for i,row in enumerate(lines):\n",
    "    txtarr[i] = row.split()\n",
    "\n",
    "# convert to feature of 1000 by 260 X_train\n",
    "\n",
    "X_train = convert_X_txt(lines,txtarr) \n",
    "y_train = np.load('badges/train/y_train_badges.npy')\n",
    "  \n",
    "    \n",
    "# load and convert test files\n",
    "    \n",
    "file = open('badges/test/test_names_badges.txt','r')\n",
    "lines = file.readlines()\n",
    "rows, cols = (len(lines), 2) \n",
    "txtarr = [[0]*cols]*rows \n",
    "for i,row in enumerate(lines):\n",
    "    txtarr[i] = row.split()\n",
    "\n",
    "X_test = convert_X_txt(lines,txtarr)\n",
    "y_test = np.load('badges/test/y_test_badges.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('badges/leaderboard/leaderboard_names_badges.txt','r')\n",
    "lines = file.readlines()\n",
    "rows,cols = (len(lines),2)\n",
    "txtarr = [[0]*cols]*rows\n",
    "for i,row in enumerate(lines):\n",
    "    txtarr[i] = row.split()\n",
    "\n",
    "X_leaderboard = convert_X_txt(lines,txtarr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('badges/hidden/hidden_names_badges.txt','r')\n",
    "lines = file.readlines()\n",
    "rows,cols = (len(lines),2)\n",
    "txtarr = [[0]*cols]*rows\n",
    "for i,row in enumerate(lines):\n",
    "    txtarr[i] = row.split()\n",
    "\n",
    "X_hidden = convert_X_txt(lines,txtarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('badges/train/train.names.txt','r')\n",
    "# lines = file.readlines()\n",
    "# rows, cols = (len(lines), 2) \n",
    "# txtarr = [[0]*cols]*rows \n",
    "# for i,row in enumerate(lines):\n",
    "#     txtarr[i] = row.split()\n",
    "#     print(txtarr[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('checkme.npy',X_train)\n",
    "# np.savetxt('checkme.csv', X_train, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_training_accuracy, hybrid_testing_accuracy, hybrid_model = train_hybrid_model(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.598\n",
      "0.545\n"
     ]
    }
   ],
   "source": [
    "print(hybrid_training_accuracy)\n",
    "print(hybrid_testing_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra credit checking vowels only on first five character in first and last name\n",
    "# all extra credit are in this block\n",
    "\n",
    "file = open('badges/train/train.names.txt','r')\n",
    "lines = file.readlines()\n",
    "rows, cols = (len(lines), 2) \n",
    "txtarr = [[0]*cols]*rows \n",
    "for i,row in enumerate(lines):\n",
    "    txtarr[i] = row.split()\n",
    "\n",
    "# convert to feature of 1000 by 50 X_train\n",
    "\n",
    "X_train = ec_convert(lines,txtarr) \n",
    "y_train = np.load('badges/train/y_train_badges.npy')\n",
    "\n",
    "# load and convert test files\n",
    "    \n",
    "file = open('badges/test/test_names_badges.txt','r')\n",
    "lines = file.readlines()\n",
    "rows, cols = (len(lines), 2) \n",
    "txtarr = [[0]*cols]*rows \n",
    "for i,row in enumerate(lines):\n",
    "    txtarr[i] = row.split()\n",
    "\n",
    "X_test = ec_convert(lines,txtarr)\n",
    "y_test = np.load('badges/test/y_test_badges.npy')\n",
    "\n",
    "\n",
    "file = open('badges/leaderboard/leaderboard_names_badges.txt','r')\n",
    "lines = file.readlines()\n",
    "rows,cols = (len(lines),2)\n",
    "txtarr = [[0]*cols]*rows\n",
    "for i,row in enumerate(lines):\n",
    "    txtarr[i] = row.split()\n",
    "X_leaderboard = ec_convert(lines,txtarr)\n",
    "\n",
    "file = open('badges/hidden/hidden_names_badges.txt','r')\n",
    "lines = file.readlines()\n",
    "rows,cols = (len(lines),2)\n",
    "txtarr = [[0]*cols]*rows\n",
    "for i,row in enumerate(lines):\n",
    "    txtarr[i] = row.split()\n",
    "X_hidden = ec_convert(lines,txtarr)\n",
    "\n",
    "Leaderboard_predict = test_unknown_data(X_train, y_train, X_leaderboard)\n",
    "filename = 'labels_ec_leaderboard'\n",
    "np.savetxt(\"{}.txt\".format(filename), Leaderboard_predict, fmt='%i', newline=\"\\n\")\n",
    "\n",
    "Hidden_predict = test_unknown_data(X_train,y_train,X_hidden)\n",
    "filename = 'labels_ec_hidden'\n",
    "np.savetxt(\"{}.txt\".format(filename), Leaderboard_predict, fmt='%i', newline=\"\\n\")\n",
    "\n",
    "\n",
    "# X_test = X_leaderboard\n",
    "\n",
    "# M = X_train.shape[1]      #number of features\n",
    "# tree_num = 100      #number of trees as features\n",
    "# # initialize a N by tree_num as new X_train made of outputs from 100 trees\n",
    "# X_train_treefeature = np.empty([X_train.shape[0],tree_num])   \n",
    "# forest = []\n",
    "# feature_indexes = np.empty([tree_num,int(M/2)])\n",
    "# # train 100 stumps per example as features\n",
    "# for i in range(tree_num):\n",
    "#     index_vec = list(range(M))\n",
    "#     for j in range(int(M/2)):\n",
    "#         index_vec.pop(random.randrange(0,int(M/2)))\n",
    "#     X_train_sub = np.delete(X_train,index_vec,1)\n",
    "#     newstump = DecisionTreeClassifier(criterion='entropy',max_depth=4)\n",
    "#     newstump = newstump.fit(X_train_sub,y_train)\n",
    "#     X_train_treefeature[:,i] = newstump.predict(X_train_sub)   #output x predictions\n",
    "#     forest.append(newstump)     #output new tree stump classifier\n",
    "#     feature_indexes[i,:] = index_vec      #output feature indexes for this tree\n",
    "\n",
    "# # apply the stump modified X_train to the sgd classifier\n",
    "# sgd_hybrid = SGDClassifier(loss = 'log', max_iter = 10000)\n",
    "# sgd_hybrid = sgd_hybrid.fit(X_train_treefeature,y_train)\n",
    "\n",
    "# # convert X_test to treefeature and test accuracy on test set\n",
    "# X_test_treefeature = np.empty([X_test.shape[0],tree_num])\n",
    "# index_vec = feature_indexes.astype(int)\n",
    "\n",
    "# # print(np.shape(index_vec))\n",
    "\n",
    "# for i in range(tree_num):\n",
    "#     X_test_sub = np.delete(X_test,index_vec[i,:],1)\n",
    "# #     print(np.shape(X_test_sub))\n",
    "#     X_test_treefeature[:,i] = forest[i].predict(X_test_sub)\n",
    "# sgd_hybrid.predict(X_test_treefeature)\n",
    "\n",
    "# #     train_accuracy = sgd_hybrid.score(X_train_treefeature,y_train)\n",
    "# test_predict_result = sgd_hybrid.predict(X_test_treefeature)\n",
    "\n",
    "# filename = 'labels_ec_leaderboard'\n",
    "# np.savetxt(\"{}.txt\".format(filename), test_predict_result, fmt='%i', newline=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Leaderboard_predict = test_unknown_data(X_train, y_train, X_leaderboard)\n",
    "filename = 'labels_2b_leaderboard'\n",
    "np.savetxt(\"{}.txt\".format(filename), Leaderboard_predict, fmt='%i', newline=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hidden_predict = test_unknown_data(X_train,y_train,X_hidden)\n",
    "filename = 'labels_2b_hidden'\n",
    "np.savetxt(\"{}.txt\".format(filename), Leaderboard_predict, fmt='%i', newline=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabet_loc(char):\n",
    "#     returns the position of the character in the alphabet in a 1X26 of 0,1 array, eg 1 at array[3] means char is a C\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    index = alphabet.index(char)\n",
    "    result = np.zeros([26,])\n",
    "    result[index] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_X_txt(lines,txtarr):\n",
    "    X = np.zeros([len(lines),260])\n",
    "    for i,row in enumerate(txtarr):\n",
    "        string1 = txtarr[i][0]\n",
    "        string2 = txtarr[i][1]\n",
    "        feature_row = np.zeros(260,)\n",
    "        for j in range(5):\n",
    "            if j < len(string1):\n",
    "                feature_row[j*26:(j+1)*26] = alphabet_loc(string1[j])\n",
    "        for j in range(5):\n",
    "            if j < len(string2):\n",
    "                feature_row[(j+5)*26:(j+6)*26] = alphabet_loc(string2[j])\n",
    "        X[i,:] = feature_row\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vowel_loc(char):\n",
    "    vowel = 'aeiou'\n",
    "    result = np.zeros([5,])\n",
    "    if char in vowel:\n",
    "        index = vowel.index(char)\n",
    "        result[index] = 1\n",
    "    return result\n",
    "\n",
    "def ec_convert(lines,txtarr):\n",
    "    X = np.zeros([len(lines),50])\n",
    "    for i,row in enumerate(txtarr):\n",
    "        string1 = txtarr[i][0]\n",
    "        string2 = txtarr[i][1]\n",
    "        feature_row = np.zeros(50,)\n",
    "        for j in range(5):\n",
    "            if j < len(string1):\n",
    "                feature_row[j*5:(j+1)*5] = vowel_loc(string1[j])\n",
    "        for j in range(5):\n",
    "            if j < len(string2):\n",
    "                feature_row[(j+5)*5:(j+6)*5] = vowel_loc(string2[j])\n",
    "        X[i,:] = feature_row\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid_model(X_train, y_train, X_test, y_test):\n",
    "    # hybrid model full train and test set\n",
    "\n",
    "    M = X_train.shape[1]      #number of features\n",
    "    tree_num = 100      #number of trees as features\n",
    "    # initialize a N by tree_num as new X_train made of outputs from 100 trees\n",
    "    X_train_treefeature = np.empty([X_train.shape[0],tree_num])   \n",
    "    forest = []\n",
    "    feature_indexes = np.empty([tree_num,int(M/2)])\n",
    "    # train 100 stumps per example as features\n",
    "    for i in range(tree_num):\n",
    "        index_vec = list(range(M))\n",
    "        for j in range(int(M/2)):\n",
    "            index_vec.pop(random.randrange(0,int(M/2)))\n",
    "        X_train_sub = np.delete(X_train,index_vec,1)\n",
    "        newstump = DecisionTreeClassifier(criterion='entropy',max_depth=4)\n",
    "        newstump = newstump.fit(X_train_sub,y_train)\n",
    "        X_train_treefeature[:,i] = newstump.predict(X_train_sub)   #output x predictions\n",
    "        forest.append(newstump)     #output new tree stump classifier\n",
    "        feature_indexes[i,:] = index_vec      #output feature indexes for this tree\n",
    "\n",
    "    # apply the stump modified X_train to the sgd classifier\n",
    "    sgd_hybrid = SGDClassifier(loss = 'log', max_iter = 10000)\n",
    "    sgd_hybrid = sgd_hybrid.fit(X_train_treefeature,y_train)\n",
    "\n",
    "    # convert X_test to treefeature and test accuracy on test set\n",
    "    X_test_treefeature = np.empty([X_test.shape[0],tree_num])\n",
    "    index_vec = feature_indexes.astype(int)\n",
    "\n",
    "    for i in range(tree_num):\n",
    "        X_test_sub = np.delete(X_test,index_vec[i,:],1)\n",
    "        X_test_treefeature[:,i] = forest[i].predict(X_test_sub)\n",
    "    sgd_hybrid.predict(X_test_treefeature)\n",
    "    \n",
    "    train_accuracy = sgd_hybrid.score(X_train_treefeature,y_train)\n",
    "    test_accuracy = sgd_hybrid.score(X_test_treefeature,y_test)\n",
    "    \n",
    "    return train_accuracy, test_accuracy, sgd_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_unknown_data(X_train, y_train, X_test):\n",
    "    # hybrid model full train and test set\n",
    "\n",
    "    M = X_train.shape[1]      #number of features\n",
    "    tree_num = 100      #number of trees as features\n",
    "    # initialize a N by tree_num as new X_train made of outputs from 100 trees\n",
    "    X_train_treefeature = np.empty([X_train.shape[0],tree_num])   \n",
    "    forest = []\n",
    "    feature_indexes = np.empty([tree_num,int(M/2)])\n",
    "    # train 100 stumps per example as features\n",
    "    for i in range(tree_num):\n",
    "        index_vec = list(range(M))\n",
    "        for j in range(int(M/2)):\n",
    "            index_vec.pop(random.randrange(0,int(M/2)))\n",
    "        X_train_sub = np.delete(X_train,index_vec,1)\n",
    "        newstump = DecisionTreeClassifier(criterion='entropy',max_depth=4)\n",
    "        newstump = newstump.fit(X_train_sub,y_train)\n",
    "        X_train_treefeature[:,i] = newstump.predict(X_train_sub)   #output x predictions\n",
    "        forest.append(newstump)     #output new tree stump classifier\n",
    "        feature_indexes[i,:] = index_vec      #output feature indexes for this tree\n",
    "\n",
    "    # apply the stump modified X_train to the sgd classifier\n",
    "    sgd_hybrid = SGDClassifier(loss = 'log', max_iter = 10000)\n",
    "    sgd_hybrid = sgd_hybrid.fit(X_train_treefeature,y_train)\n",
    "\n",
    "    # convert X_test to treefeature and test accuracy on test set\n",
    "    X_test_treefeature = np.empty([X_test.shape[0],tree_num])\n",
    "    index_vec = feature_indexes.astype(int)\n",
    "    \n",
    "    print(np.shape(index_vec))\n",
    "\n",
    "    for i in range(tree_num):\n",
    "        X_test_sub = np.delete(X_test,index_vec[i,:],1)\n",
    "        print(np.shape(X_test_sub))\n",
    "        X_test_treefeature[:,i] = forest[i].predict(X_test_sub)\n",
    "    sgd_hybrid.predict(X_test_treefeature)\n",
    "    \n",
    "#     train_accuracy = sgd_hybrid.score(X_train_treefeature,y_train)\n",
    "    test_predict_result = sgd_hybrid.predict(X_test_treefeature)\n",
    "    \n",
    "    return test_predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
