{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVLSo4TR0aQU"
   },
   "source": [
    "# CIS 519 Homework 2: Linear Classifiers\n",
    "\n",
    "- Handed Out: October 5, 2020\n",
    "- Due: October 19, 2020 at 11:59pm.\n",
    "\n",
    "Although the solutions are my own, I consulted with the following people\n",
    "while working on this homework:\n",
    "- TODO (if applicable): ...\n",
    "\n",
    "## Preface\n",
    "\n",
    "- Feel free to talk to other members of the class in doing the homework. I am more concerned that you learn how to solve the problem than that you demonstrate that you solved it entirely on your own. You should, however, **write down your solution yourself**. Please include here the list of people you consulted with in the course of working on the homework:\n",
    "\n",
    "- While we encourage discussion within and outside the class, cheating and copying code is strictly not allowed. Copied code will result in the entire assignment being discarded at the very least.\n",
    "\n",
    "- Please use Piazza if you have questions about the homework. Also, please come to the TAs recitations and to the office hours.\n",
    "\n",
    "- The homework is due at 11:59 PM on the due date. We will be using Gradescope for collecting the homework assignments. You should have been automatically added to Gradescope. If not, please ask a TA for assistance. Post on Piazza and contact the TAs if you are having technical difficulties in submitting the assignment.\n",
    "\n",
    "- Here are some resources you will need for this assignment (https://www.seas.upenn.edu/~cis519/fall2020/assets/HW/HW2/hw2-materials.zip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClICZP2npLtE"
   },
   "source": [
    "# Overview\n",
    "\n",
    "### About Jupyter Notebooks\n",
    "\n",
    "In this homework assignment, we will use a Jupyter notebook to implement, analyze, and discuss ML classifiers.\n",
    "Knowing and being comfortable with Jupyter notebooks is a must in every data scientist, ML engineer, researcher, etc. They are widely used in industry and are a standard form of communication in ML by intertwining text and code to \"tell a story\". There are many resources that can introduce you to Jupyter notebooks (they are pretty easy to understand!), and if you still need help any of the TAs are more than willing to help.\n",
    "\n",
    "We will be using a local instance of Jupyter instead of Colab. You are of course free to use Colab, but you will need to understand how to hook your Colab instance with Google Drive to upload the datasets and to save images.\n",
    "\n",
    "\n",
    "\n",
    "### About the Homework\n",
    "\n",
    "You will experiment with several different linear classifiers and analyze \n",
    "their performances in both real and synthetic datasets. The goal is to understand the differences and\n",
    "similarities between the algorithms and the impact that the dataset characteristics have on the\n",
    "algorithms' learning behaviors and performances.\n",
    "\n",
    "In total, there are seven different learning algorithms which you will implement.\n",
    "Six are variants of the Perceptron algorithm and the seventh is a support vector machine (SVM).\n",
    "The details of these models is described in Section 1.\n",
    "\n",
    "\n",
    "In order to evaluate the performances of these models, you will use several different datasets.\n",
    "The first two datasets are synthetic datasets that have features and labels that were programatically\n",
    "generated. They were generated using the same script but use different input parameters that produced \n",
    "sparse and dense variants. The second two datasets are for the task of named-entity recognition (NER),\n",
    "identifying the names of people, locations, and organizations within text.\n",
    "One comes from news text and the other from a corpus of emails.\n",
    "For these two datasets, you need to implement the feature extraction yourself.\n",
    "All of the datasets and feature extraction information are described in Section 2.\n",
    "\n",
    "Finally, you will run two sets of experiments, one on the synthetic data and one on the NER data.\n",
    "The first set will analyze how the amount of training data impacts model performance.\n",
    "The second will look at the consequences of having training and testing data that come from different domains.\n",
    "The details of the experiments are described in Section 3.\n",
    "\n",
    "### Distribution of Points\n",
    "\n",
    "The homework has 4 sections for a total of 100 points + 10 extra credit points:\n",
    "- Section 0: Warmup (5 points)\n",
    "- Section 1: Linear Classifiers (30 points)\n",
    "- Section 2: Datasets (0 points, just text)\n",
    "- Section 3: Experiments (65 points)\n",
    "    - Synthetic Experiment:\n",
    "        - Parameter Tuning (10 points)\n",
    "        - Learning Curves(10 points)\n",
    "        - Final Test Accuracies (5 points)\n",
    "        - Discussion Questions (5 points)\n",
    "        - Noise Experiment (10 points **extra credit**)\n",
    "    - NER Experiment:\n",
    "        - Feature Extraction (25 points)\n",
    "        - Final Test Accuracies (5 points)\n",
    "        - $F_1$ Discussion Questions (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4gI2-Ygpr69"
   },
   "source": [
    "# Section 0: Warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ecz8xJo-ojUs"
   },
   "source": [
    "###### Only For Colab\n",
    "\n",
    "If you want to complete this homework in Colab, you are more than welcome to.\n",
    "You will need a little bit more maneuvering since you will need to upload\n",
    "the files of hw2 to your Google Drive and run the following two cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "executionInfo": {
     "elapsed": 16579,
     "status": "ok",
     "timestamp": 1601838226119,
     "user": {
      "displayName": "Samuel Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKvzcGAASWuvxjrEp7uMipUJWbOgy2JiGGaHeOtg=s64",
      "userId": "11877295110173304666"
     },
     "user_tz": 240
    },
    "id": "PUMTPLGG-3s2",
    "outputId": "9f08f1c8-19ed-4983-e733-5586d32d5649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you want to use Colab for this homework.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1601838234837,
     "user": {
      "displayName": "Samuel Xu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKvzcGAASWuvxjrEp7uMipUJWbOgy2JiGGaHeOtg=s64",
      "userId": "11877295110173304666"
     },
     "user_tz": 240
    },
    "id": "eL4xKfSbop6z",
    "outputId": "d7f3736e-791d-466e-d836-c71d44d2e87d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you want to use Colab for this homework.\n",
    "# %cd /content/drive/My Drive/Colab Notebooks/YOUR_PATH_TO_HW_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwXv78-V_wL_"
   },
   "source": [
    "###### Python Version\n",
    "\n",
    "Python 3.6 or above is required for this homework. Make sure you have it installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "T5ANvqaM_wMA"
   },
   "outputs": [],
   "source": [
    "# Let's check.\n",
    "import sys\n",
    "if sys.version_info[:2] < (3, 6):\n",
    "    raise Exception(\"You have Python version \" + str(sys.version_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0fVHRxp0aQX"
   },
   "source": [
    "## Imports and Helper Functions (5 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2feRgjW0aQY"
   },
   "source": [
    "Let's import useful modules we will need throughout the homework\n",
    "as well as implement helper functions for our experiment. **Read and remember** what each function is doing, as you will probably need some of them down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ah78siKM0aQZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\tongd\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tongd\\anaconda3\\lib\\site-packages (from sklearn) (0.23.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\tongd\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tongd\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\tongd\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\tongd\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tongd\\anaconda3\\lib\\site-packages (3.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\tongd\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\tongd\\anaconda3\\lib\\site-packages (from matplotlib) (1.18.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\tongd\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\tongd\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tongd\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tongd\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\tongd\\anaconda3\\lib\\site-packages (1.18.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries for this homework. only need to run once i think\n",
    "%pip install sklearn\n",
    "%pip install matplotlib\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s4IV1W4y0aQg"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "DATASETS_PATH = \"datasets/\"\n",
    "NER_PATH = os.path.join(DATASETS_PATH, 'ner')\n",
    "SYNTHETIC_PATH = os.path.join(DATASETS_PATH, 'synthetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UdcTYi0w0aQl"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function that loads a synthetic dataset from the dataset root (e.g. \"synthetic/sparse\").\n",
    "\n",
    "You should not need to edit this method.\n",
    "\"\"\"\n",
    "def load_synthetic_data(dataset_type):\n",
    "\n",
    "    def load_jsonl(file_path):\n",
    "        data = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "        return data\n",
    "\n",
    "    def load_txt(file_path):\n",
    "        data = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data.append(int(line.strip()))\n",
    "        return data\n",
    "\n",
    "    def convert_to_sparse(X):\n",
    "        sparse = []\n",
    "        for x in X:\n",
    "            data = {}\n",
    "            for i, value in enumerate(x):\n",
    "                if value != 0:\n",
    "                    data[str(i)] = value\n",
    "            sparse.append(data)\n",
    "        return sparse\n",
    "\n",
    "    path = os.path.join(SYNTHETIC_PATH, dataset_type)\n",
    "    \n",
    "    X_train = load_jsonl(os.path.join(path, 'train.X'))\n",
    "    X_dev = load_jsonl(os.path.join(path, 'dev.X'))\n",
    "    X_test = load_jsonl(os.path.join(path, 'test.X'))\n",
    "\n",
    "    num_features = len(X_train[0])\n",
    "    features = [str(i) for i in range(num_features)]\n",
    "\n",
    "    X_train = convert_to_sparse(X_train)\n",
    "    X_dev = convert_to_sparse(X_dev)\n",
    "    X_test = convert_to_sparse(X_test)\n",
    "\n",
    "    y_train = load_txt(os.path.join(path, 'train.y'))\n",
    "    y_dev = load_txt(os.path.join(path, 'dev.y'))\n",
    "    y_test = load_txt(os.path.join(path, 'test.y'))\n",
    "\n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rYO0o0VB0aQp"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function that loads the NER data from a path (e.g. \"ner/conll/train\"). \n",
    "\n",
    "You should not need to edit this method.\n",
    "\"\"\"\n",
    "def load_ner_data(dataset=None, dataset_type=None):\n",
    "    # List of tuples for each sentence\n",
    "    data = []\n",
    "    path = os.path.join(os.path.join(NER_PATH, dataset), dataset_type)\n",
    "    for filename in os.listdir(path):\n",
    "        with open(os.path.join(path, filename), 'r') as file:\n",
    "            sentence = []\n",
    "            for line in file:\n",
    "                if line == '\\n':\n",
    "                    data.append(sentence)\n",
    "                    sentence = []\n",
    "                else:\n",
    "                    sentence.append(tuple(line.split()))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YYpjru5b0aQu"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A helper function that plots the relationship between number of examples\n",
    "and accuracies for all the models.\n",
    "\n",
    "You should not need to edit this method.\n",
    "\"\"\"\n",
    "def plot_learning_curves(\n",
    "    perceptron_accs,\n",
    "    winnow_accs,\n",
    "    adagrad_accs,\n",
    "    avg_perceptron_accs,\n",
    "    avg_winnow_accs,\n",
    "    avg_adagrad_accs,\n",
    "    svm_accs\n",
    "):\n",
    "    \"\"\"\n",
    "    This function will plot the learning curve for the 7 different models.\n",
    "    Pass the accuracies as lists of length 11 where each item corresponds\n",
    "    to a point on the learning curve.\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracies = [\n",
    "        ('perceptron', perceptron_accs),\n",
    "        ('winnow', winnow_accs),\n",
    "        ('adagrad', adagrad_accs),\n",
    "        ('avg-perceptron', avg_perceptron_accs),\n",
    "        ('avg-winnow', avg_winnow_accs),\n",
    "        ('avg-adagrad', avg_adagrad_accs),\n",
    "        ('svm', svm_accs)\n",
    "    ]\n",
    "    \n",
    "    x = [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 10000]\n",
    "    plt.figure()\n",
    "    f, (ax, ax2) = plt.subplots(1, 2, sharey=True, facecolor='w')\n",
    "    \n",
    "    for label, acc_list in accuracies:\n",
    "        assert len(acc_list) == 11\n",
    "        ax.plot(x, acc_list, label=label)\n",
    "        ax2.plot(x, acc_list, label=label)\n",
    "        \n",
    "    ax.set_xlim(0, 5500)\n",
    "    ax2.set_xlim(9500, 10000)\n",
    "    ax2.set_xticks([10000])\n",
    "    # hide the spines between ax and ax2\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax2.spines['left'].set_visible(False)\n",
    "    ax.yaxis.tick_left()\n",
    "    ax.tick_params(labelright='off')\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sg9QfIak0aQz"
   },
   "source": [
    "### F1 Score (5 points)\n",
    "\n",
    "For some part of the homework, you will use the F1 score instead of accuracy to evaluate how well a model does. The F1 score is computed as the harmonic mean of the precision and recall of the classifier. Precision measures the number of correctly identified positive results by the total number of positive results. Recall, on the other hand, measures the number of correctly identified positive results divided by the number of all samples that should have been identified as positive. More formally, we have that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Precision} &= \\frac{TP}{TP + FP} \\\\\n",
    "\\text{Recall} &= \\frac{TP}{TP + FN}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $TP$ is the number of true positives, $FP$ false positives and $FN$ false negatives. Combining these two we define F1 as\n",
    "\n",
    "$$\n",
    "\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "You now need to implement the calculation of F1 yourself using the provided function header. It will be unit tested on Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WnzC-Gce0aQ0"
   },
   "outputs": [],
   "source": [
    "def calculate_f1(y_gold, y_model):\n",
    "    \"\"\"\n",
    "    TODO: MODIFY \n",
    "    \n",
    "    Computes the F1 of the model predictions using the\n",
    "    gold labels. Each of y_gold and y_model are lists with\n",
    "    labels 1 or -1. The function should return the F1\n",
    "    score as a number between 0 and 1.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    diff = np.zeros(len(y_gold))\n",
    "    for i in range(len(y_gold)):\n",
    "        diff[i] = y_gold[i] - y_model[i]\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for i in range(len(diff)):\n",
    "        if diff[i] == 0 and y_gold[i] == 1:\n",
    "            TP += 1\n",
    "        if diff[i] > 0:\n",
    "            FN += 1\n",
    "        if diff[i] < 0:\n",
    "            FP += 1\n",
    "    \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Pbt52HH0aQ7"
   },
   "source": [
    "Looking at the formula for the F1 score, what is the highest and lowest possible value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jB4UrDGR0aQ8"
   },
   "outputs": [],
   "source": [
    "def highest_and_lowest_f1_score():\n",
    "    \"\"\"\n",
    "    TODO: MODIFY\n",
    "    \n",
    "    Return the highest and lowest possible F1 score\n",
    "    (ie one line solution returning the theoretical max and min)\n",
    "    \"\"\"\n",
    "    \n",
    "    maxscore = 1\n",
    "    minscore = 0\n",
    "    \n",
    "    return maxscore, minscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS0BO0AJ0aRA"
   },
   "source": [
    "# Section 1. Linear Classifiers (30 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZM2eSe030aRB"
   },
   "source": [
    "This section details the seven different algorithms that you will use in the experiments. For each of the algorithms, we describe the initialization you should use to start training and the different parameter settings that you should use for the experiment on the synthetic datasets. Each of the update functions for the Perceptron, Winnow, and Perceptron with AdaGrad will be unittested on Gradescope, so please do not edit the function definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YahdyT4H0aRC"
   },
   "source": [
    "### 1.1 Base Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kRcseeXa0aRD"
   },
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    \"\"\"\n",
    "    DO NOT MODIFY\n",
    "\n",
    "    The Classifier class is the base class for all of the Perceptron-based\n",
    "    algorithms. Your class should override the \"process_example\" and\n",
    "    \"predict_single\" functions. Further, the averaged models should\n",
    "    override the \"finalize\" method, where the final parameter values\n",
    "    should be calculated. \n",
    "    \n",
    "    You should not need to edit this class any further.\n",
    "    \"\"\"\n",
    "    \n",
    "    ITERATIONS = 10\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        for iteration in range(self.ITERATIONS):\n",
    "            for x_i, y_i in zip(X, y):\n",
    "                self.process_example(x_i, y_i)\n",
    "        self.finalize()\n",
    "\n",
    "    def process_example(self, x, y):\n",
    "        \"\"\"\n",
    "        Makes a prediction using the current parameter values for\n",
    "        the features x and potentially updates the parameters based\n",
    "        on the gradient. Note \"x\" is a dictionary which maps from the feature\n",
    "        name to the feature value and y is either 1 or -1.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"Calculates the final parameter values for the averaged models.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts labels for all of the input examples. You should not need\n",
    "        to override this method.\n",
    "        \"\"\"\n",
    "        return [self.predict_single(x) for x in X]\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        \"\"\"\n",
    "        Predicts a label, 1 or -1, for the input example. \"x\" is a dictionary\n",
    "        which maps from the feature name to the feature value.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzRKTNaK0aRH"
   },
   "source": [
    "### 1.2 Basic Perceptron (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVvMIuX10aRI"
   },
   "source": [
    "We do this classifier for you, so enjoy the two free points and pay attention to the techniques and code written.\n",
    "\n",
    "#### 1.2.1 Description\n",
    "\n",
    "This is the basic version of the Perceptron Algorithm.\n",
    "    In this version, an update will be performed on the example $(\\textbf{x}, y)$ if $y(\\mathbf{w}^\\intercal \\mathbf{x} + \\theta) \\leq 0$. The Perceptron needs to learn both the bias term $\\theta$ and the weight vector $\\mathbf{w}$ parameters.\n",
    "When the Perceptron makes a mistake on the example $(\\textbf{x}, y)$, both $\\mathbf{w}$ and $\\theta$ need to be updated using the following update equations:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{w}^\\textrm{new} &\\gets \\mathbf{w} + \\eta \\cdot y \\cdot \\mathbf{x} \\\\\n",
    "    \\theta^\\textrm{new} &\\gets \\theta + \\eta \\cdot y\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "#### 1.2.2 Hyperparameters\n",
    "\n",
    "We set $\\eta$ to 1, so there are no hyperparameters to tune. \n",
    "\n",
    "Note: If we assume that the order of the examples presented to the algorithm is fixed, we initialize $\\mathbf{w} = \\mathbf{0}$ and $\\theta = 0$, and train both together, then the learning rate $\\eta$ does not have any effect.\n",
    "        In fact you can show that, if $\\mathbf{w}_1$ and $\\theta_1$ are the outputs of the Perceptron algorithm with learning rate $\\eta_1$, then $\\mathbf{w}_1/\\eta_1$ and $\\theta_1/\\eta_1$ will be the result of the Perceptron with learning rate 1 (note that these two hyperplanes give identical predictions).\n",
    "\n",
    "#### 1.2.3 Initialization\n",
    "\n",
    "$\\mathbf{w} = [0, 0, \\dots, 0]$ and $\\theta = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zotWbBvj0aRJ"
   },
   "outputs": [],
   "source": [
    "class Perceptron(Classifier):\n",
    "    \"\"\"\n",
    "    DO NOT MODIFY THIS CELL\n",
    "\n",
    "    The Perceptron model. Note how we are subclassing `Classifier`.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features):\n",
    "        \"\"\"\n",
    "        Initializes the parameters for the Perceptron model. \"features\"\n",
    "        is a list of all of the features of the model where each is\n",
    "        represented by a string.\n",
    "        \"\"\"\n",
    "        \n",
    "        # NOTE: Do not change the names of these 3 data members because\n",
    "        # they are used in the unit tests\n",
    "        self.eta = 1\n",
    "        self.theta = 0\n",
    "        self.w = {feature: 0.0 for feature in features}\n",
    "\n",
    "    def process_example(self, x, y):\n",
    "        y_pred = self.predict_single(x)\n",
    "        if y != y_pred:\n",
    "            for feature, value in x.items():\n",
    "                self.w[feature] += self.eta * y * value\n",
    "            self.theta += self.eta * y\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        score = 0\n",
    "        for feature, value in x.items():\n",
    "            score += self.w[feature] * value\n",
    "        score += self.theta\n",
    "        if score <= 0:\n",
    "            return -1\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaiyRKjo0aRN"
   },
   "source": [
    "For the rest of the Perceptron-based algorithms, you will have to implement the corresponding class like we have done for `Perceptron`.\n",
    "Use the `Perceptron` class as a guide for how to implement the functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWiuhyOm0aRO"
   },
   "source": [
    "### 1.3 Winnow (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9dx93VU0aRR"
   },
   "source": [
    "#### 1.3.1 Description\n",
    "The Winnow algorithm is a variant of the Perceptron algorithm with multiplicative updates. Since the algorithm requires that the target function is monotonic, you will only use it on the synthetic datasets.\n",
    "\n",
    "The Winnow algorithm only learns parameters $\\mathbf{w}$.\n",
    "We will fix $\\theta = -n$, where $n$ is the number of features.\n",
    "When the Winnow algorithm makes a mistake on the example $(\\textbf{x}, y)$, the parameters are updated with the following equation:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    w^\\textrm{new}_i \\gets w_i \\cdot \\alpha^{y \\cdot x_i}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $w_i$ and $x_i$ are the $i$th components of the corresponding vectors.\n",
    "Here, $\\alpha$ is a promotion/demotion hyperparameter.\n",
    "\n",
    "#### 1.3.2 Hyperparameters\n",
    "\n",
    "For the experiment, choose $\\alpha \\in \\{1.1, 1.01, 1.005, 1.0005, 1.0001\\}$.\n",
    "\n",
    "#### 1.3.3 Initialization\n",
    "\n",
    "$\\mathbf{w} = [1, 1, \\dots, 1]$ and $\\theta = -n$ (constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "g9pPUPTd0aRS"
   },
   "outputs": [],
   "source": [
    "class Winnow(Classifier):\n",
    "    def __init__(self, alpha, features):\n",
    "        # DO NOT change the names of these 3 data members because\n",
    "        # they are used in the unit tests\n",
    "        self.alpha = alpha\n",
    "        self.w = {feature: 1.0 for feature in features}\n",
    "        self.theta = -len(features)\n",
    "        \n",
    "    def process_example(self, x, y):\n",
    "        \"\"\" TODO: IMPLEMENT\"\"\"\n",
    "        \n",
    "        y_pred = self.predict_single(x)\n",
    "        if y != y_pred:\n",
    "            for feature, value in x.items():\n",
    "                self.w[feature] = self.w[feature] * pow(self.alpha,y*value)\n",
    "        \n",
    "\n",
    "    def predict_single(self, x):\n",
    "        \"\"\" TODO: IMPLEMENT\"\"\"\n",
    "        \n",
    "        score = 0\n",
    "        for feature, value in x.items():\n",
    "            score += self.w[feature] * value\n",
    "        score += self.theta\n",
    "        \n",
    "        if score <= 0:\n",
    "            return -1\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--I-w3_U0aRX"
   },
   "source": [
    "### 1.4 AdaGrad (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5AXEuKn0aRY"
   },
   "source": [
    "#### 1.4.1 Description\n",
    "AdaGrad is a variant of the Perceptron algorithm that adapts the learning rate for each parameter based on historical information.\n",
    "    The idea is that frequently changing features get smaller learning rates and stable features higher ones.\n",
    "\n",
    "To derive the update equations for this model, we first need to start with the loss function.\n",
    "Instead of using the hinge loss with the elbow at 0 (like the basic Perceptron does), we will instead use the standard hinge loss with the elbow at 1:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(\\mathbf{x}, y, \\mathbf{w}, \\theta) = \\max\\left\\{0, 1 - y(\\mathbf{w}^\\intercal \\mathbf{x} + \\theta)\\right\\}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Then, by taking the partial derivative of $\\mathcal{L}$ with respect to $\\mathbf{w}$ and $\\theta$, we can derive the respective graidents (make sure you understand how you could derive these gradients on your own):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{w}} &=\n",
    "        \\begin{cases}\n",
    "        \\mathbf{0} & \\text{if $y(\\mathbf{w}^\\intercal \\mathbf{x} + \\theta) > 1$} \\\\\n",
    "        -y\\cdot \\mathbf{x} & \\textrm{otherwise}\n",
    "        \\end{cases} \\\\\n",
    "    \\frac{\\partial\\mathcal{L}}{\\partial\\theta} &=\n",
    "        \\begin{cases}\n",
    "            0 & \\text{if $y(\\mathbf{w}^\\intercal \\mathbf{x} + \\theta) > 1$} \\\\\n",
    "            -y & \\textrm{otherwise}\n",
    "        \\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then for each parameter, we will keep track of the sum of the parameters' squared gradients.\n",
    "In the following equations, the $k$ superscript refers to the $k$th non-zero gradient (i.e., the $k$th weight vector/misclassified example) and $t$ is the number of mistakes seen thus far.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    G^t_j &= \\sum_{k=1}^t \\left(\\frac{\\partial \\mathcal{L}}{\\partial w^k_j}\\right)^2 \\\\\n",
    "    H^t &= \\sum_{k=1}^t \\left(\\frac{\\partial \\mathcal{L}}{\\partial \\theta^k}\\right)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For example, on the 3rd mistake ($t = 3$), $G^3_j$ is the sum of the squares of the first three non-zero gradients ($\\left(\\frac{\\partial \\mathcal{L}}{\\partial w^1_j}\\right)^2$, $\\left(\\frac{\\partial \\mathcal{L}}{\\partial w^2_j}\\right)^2$, and $\\left(\\frac{\\partial \\mathcal{L}}{\\partial w^3_j}\\right)^2$).\n",
    "Then $\\mathbf{G}^3$ is used to calculate the 4th value of the weight vector as follows.\n",
    "On example $(\\mathbf{x}, y)$, if $y(\\mathbf{w}^\\intercal \\mathbf{x} + \\theta) \\leq 1$, then the parameters are updated with the following equations:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{w}^{t+1} &\\gets \\mathbf{w}^t + \\eta \\cdot \\frac{y \\cdot \\mathbf{x}}{\\sqrt{\\mathbf{G}^t}} \\\\\n",
    "    \\theta^{t+1} &\\gets \\theta^t + \\eta \\frac{y}{\\sqrt{H^t}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that, although we use the hinge loss with the elbow at 1 for training, you still make the prediction based on whether or not $y(\\mathbf{w}^\\intercal \\mathbf{x} + \\theta) \\leq 0$ during testing.\n",
    "\n",
    "#### 1.4.2 Hyperparameters\n",
    "\n",
    "For the experiment, choose $\\eta \\in \\{1.5, 0.25, 0.03, 0.005, 0.001\\}$\n",
    "\n",
    "#### 1.4.3 Initialization\n",
    "\n",
    "$\\mathbf{w} = [0, 0, \\dots, 0]$ and $\\theta = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "w50RXTza0aRZ"
   },
   "outputs": [],
   "source": [
    "class AdaGrad(Classifier):\n",
    "    def __init__(self, eta, features):\n",
    "        # DO NOT change the names of these 3 data members because\n",
    "        # they are used in the unit tests\n",
    "        self.eta = eta\n",
    "        self.w = {feature: 0.0 for feature in features}\n",
    "        self.theta = 0\n",
    "        self.G = {feature: 1e-5 for feature in features}  # 1e-5 prevents divide by 0 problems\n",
    "        self.H = 0\n",
    "        \n",
    "    def process_example(self, x, y):\n",
    "        \"\"\" TODO: IMPLEMENT\"\"\"\n",
    "        import numpy as np\n",
    "        \n",
    "        y_pred = self.predict_single(x)\n",
    "        dotpro = 0\n",
    "        \n",
    "        if y != y_pred:\n",
    "            \n",
    "#             calculate dot product\n",
    "            for feature, value in x.items():\n",
    "                dotpro += self.w[feature] * value\n",
    "                \n",
    "#             update w\n",
    "            for feature, value in x.items():\n",
    "                if(y * (dotpro + self.theta) > 1):\n",
    "                    dldw = 0                   \n",
    "                else:\n",
    "                    dldw = -y * value\n",
    "                self.G[feature] += dldw ** 2\n",
    "                self.w[feature] += self.eta * y * value / np.sqrt(self.G[feature])\n",
    "                \n",
    "#             update theta        \n",
    "            if(y * (dotpro + self.theta) > 1):\n",
    "                dldtheta = 0\n",
    "            else:\n",
    "                dldtheta = -y\n",
    "            self.H += dldtheta ** 2\n",
    "            self.theta += self.eta * y / np.sqrt(self.H)  \n",
    "        \n",
    "\n",
    "    def predict_single(self, x):\n",
    "        \"\"\" TODO: IMPLEMENT\"\"\"\n",
    "\n",
    "        score = 0\n",
    "        for feature, value in x.items():\n",
    "            score += self.w[feature] * value\n",
    "        score += self.theta\n",
    "        \n",
    "        if score <= 0:\n",
    "            return -1\n",
    "        return 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmAio9O40aRd"
   },
   "source": [
    "### 1.5 Averaged Models (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzPnbRJ60aRe"
   },
   "source": [
    "You will also implement the averaged version of the previous three algorithms.\n",
    "\n",
    "During the course of training, each of the above algorithms will have $K + 1$ different parameter settings for the $K$ different updates it will make during training.\n",
    "The regular implementation of these algorithms uses the parameter values after the $K$th update as the final ones.\n",
    "Instead, the averaged version use the weighted average of the $K + 1$ parameter values as the final parameter values.\n",
    "Let $m_k$ denote the number of correctly classified examples by the $k$th parameter values and $M$ the total number of correctly classified examples.\n",
    "The final parameter values are\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    M &= \\sum_{k=1}^{K+1} m_k \\\\\n",
    "    \\mathbf{w} &\\gets \\frac{1}{M} \\sum_{k=1}^{K+1} m_k \\cdot \\mathbf{w}^k \\\\\n",
    "    \\theta &\\gets \\frac{1}{M} \\sum_{k=1}^{K+1} m_k \\cdot \\theta^k \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For each of the averaged versions of Perceptron, Winnow, and AdaGrad, use the same hyperparameters and initialization as before.\n",
    "\n",
    "#### 1.5.1 Implementation Note\n",
    "Implementing the averaged variants of these algorithms can be tricky.\n",
    "    While the final parameter values are based on the sum of $K$ different vectors, there is no need to maintain *all* of these parameters.\n",
    "    Instead, you should implement these algorithms by keeping only two vectors, one which maintains the cumulative sum and the current one.\n",
    "\n",
    "Additionally, there are two ways of keeping track of these two vectors.\n",
    "One is more straightforward but prohibitively slow.\n",
    "The second requires some algebra to derive but is significantly faster to run.\n",
    "Try to analyze how the final weight vector is a function of the intermediate updates and their corresponding weights.\n",
    "It should take less than a minute or two for ten iterations for any of the averaged algorithms.\n",
    "**You need to think about how to efficiently implement the averaged algorithms yourself.**\n",
    "\n",
    "Further, the implementation for Winnow is slightly more complicated than the other two, so if you consistently have low accuracy for the averaged Winnow, take a closer look at the derivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "Vo-NP3ot0aRf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AveragedPerceptron(Classifier):\n",
    "    def __init__(self, features):\n",
    "        self.eta = 1\n",
    "        self.w = {feature: 0.0 for feature in features}\n",
    "        self.theta = 0\n",
    "        \"\"\"TODO: You will need to add data members here\"\"\"\n",
    "        self.M = 0\n",
    "        self.lastM = 0\n",
    "        self.weightedW = {feature: 1.0 for feature in features}\n",
    "        self.weightedT = 0     \n",
    "        \n",
    "    def process_example(self, x, y):\n",
    "        \"\"\" TODO: IMPLEMENT\"\"\"\n",
    "        y_pred = self.predict_single(x)\n",
    "        if y != y_pred:\n",
    "            \n",
    "#             update sum of mk*wk and mk*thetak\n",
    "            for feature in self.weightedW:\n",
    "                self.weightedW[feature] += (self.M - self.lastM) * self.w[feature]\n",
    "            self.weightedT += (self.M - self.lastM) * self.theta\n",
    "            \n",
    "#             update weights\n",
    "            for feature, value in x.items():\n",
    "                self.w[feature] += self.eta * y * value\n",
    "            self.theta += self.eta * y\n",
    "            \n",
    "#             reset lastM\n",
    "#             self.M += 1\n",
    "            self.lastM = self.M\n",
    "            \n",
    "        else:\n",
    "            self.M += 1\n",
    "        \n",
    "    def predict_single(self, x):\n",
    "        \"\"\" TODO: IMPLEMENT\"\"\"\n",
    "        \n",
    "        score = 0\n",
    "        for feature, value in x.items():\n",
    "            score += self.w[feature] * value\n",
    "        score += self.theta\n",
    "        if score <= 0:\n",
    "            return -1\n",
    "        return 1  \n",
    "        \n",
    "    def finalize(self):\n",
    "        \"\"\" TODO: IMPLEMENT\"\"\"\n",
    "        self.M +=1\n",
    "        for feature in self.weightedW:\n",
    "            self.w[feature] = 1/self.M * self.weightedW[feature]\n",
    "        self.theta = 1/self.M * self.weightedT\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9445\n"
     ]
    }
   ],
   "source": [
    "# test the averaged perceptron model\n",
    "\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test, features = load_synthetic_data('dense')\n",
    "\n",
    "AvgPerceptron = AveragedPerceptron(features)\n",
    "iteration = 10\n",
    "\n",
    "for iter in range(iteration):\n",
    "    for i in range(len(X_train)):\n",
    "        AvgPerceptron.process_example(X_train[i],y_train[i])\n",
    "AvgPerceptron.finalize()\n",
    "\n",
    "count = 0\n",
    "for i in range(len(X_dev)):\n",
    "    x = X_dev[i]\n",
    "    y = AvgPerceptron.predict_single(x)\n",
    "    if y_dev[i] == y:\n",
    "        count += 1\n",
    "print(count/len(X_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "bv-kvuF30aRj"
   },
   "outputs": [],
   "source": [
    "class AveragedWinnow(Classifier):\n",
    "    def __init__(self, alpha, features):\n",
    "        self.alpha = alpha\n",
    "        self.w = {feature: 1.0 for feature in features}\n",
    "        self.theta = -len(features)\n",
    "        \"\"\"TODO: You will need to add data members here\"\"\"\n",
    "        self.M = 0\n",
    "        self.lastM = 0\n",
    "        self.weightedW = {feature: 1.0 for feature in features}\n",
    "\n",
    "        \n",
    "    def process_example(self, x, y):\n",
    "        \"\"\" TODO: IMPLEMENT\"\"\"\n",
    "              \n",
    "        y_pred = self.predict_single(x)\n",
    "        if y != y_pred:\n",
    "            for feature, value in x.items():\n",
    "                self.w[feature] = self.w[feature] * pow(self.alpha,y*value)\n",
    "                \n",
    "            for feature in self.weightedW:\n",
    "                self.weightedW[feature] += (self.M - self.lastM) * self.w[feature]\n",
    "            \n",
    "#             self.M += 1\n",
    "            self.lastM = self.M\n",
    "        else:\n",
    "            self.M +=1\n",
    "\n",
    "\n",
    "    def predict_single(self, x):\n",
    "        \"\"\" TODO: IMPLEMENT\"\"\"\n",
    "        score = 0\n",
    "        for feature, value in x.items():\n",
    "            score += self.w[feature] * value\n",
    "        score += self.theta\n",
    "        \n",
    "        if score <= 0:\n",
    "            return -1\n",
    "        return 1\n",
    "        \n",
    "    def finalize(self):\n",
    "        \"\"\" TODO: IMPLEMENT\"\"\"\n",
    "        self.M += 1\n",
    "        for feature in self.weightedW:\n",
    "            self.w[feature] = 1/self.M * self.weightedW[feature]        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.939\n"
     ]
    }
   ],
   "source": [
    "# test the averaged Winnow model\n",
    "\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test, features = load_synthetic_data('sparse')\n",
    "\n",
    "iteration = 10\n",
    "alpha = 1.1\n",
    "AvgWinnow = AveragedWinnow(alpha,features)\n",
    "AvgWinnow.train(X_train,y_train)\n",
    "\n",
    "# AvgWinnow = Winnow(alpha,features)\n",
    "# AvgWinnow.train(X_train,y_train)\n",
    "\n",
    "count = 0\n",
    "for i in range(len(X_dev)):\n",
    "    x = X_dev[i]\n",
    "    y = AvgWinnow.predict_single(x)\n",
    "    if y_dev[i] == y:\n",
    "        count += 1\n",
    "print(count/len(X_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "L9xWrGHS0aRn"
   },
   "outputs": [],
   "source": [
    "class AveragedAdaGrad(Classifier):\n",
    "    def __init__(self, eta, features):\n",
    "        self.eta = eta\n",
    "        self.w = {feature: 0.0 for feature in features}\n",
    "        self.theta = 0\n",
    "        self.G = {feature: 1e-5 for feature in features}\n",
    "        self.H = 0\n",
    "        \"\"\"TODO: You will need to add data members here\"\"\"\n",
    "        self.M = 0\n",
    "        self.lastM = 0\n",
    "        self.weightedW = {feature: 1.0 for feature in features}\n",
    "        self.weightedT = 0\n",
    "        \n",
    "    def process_example(self, x, y):\n",
    "        \"\"\" TODO: IMPLEMENT\"\"\"\n",
    "        import numpy as np\n",
    "        \n",
    "        y_pred = self.predict_single(x)\n",
    "        dotpro = 0\n",
    "        \n",
    "        if y != y_pred:\n",
    "            \n",
    "            for feature in self.weightedW:\n",
    "                self.weightedW[feature] += (self.M - self.lastM) * self.w[feature]\n",
    "            self.weightedT += (self.M - self.lastM) * self.theta\n",
    "            \n",
    "#             calculate dot product\n",
    "            for feature, value in x.items():\n",
    "                dotpro += self.w[feature] * value\n",
    "                \n",
    "#             update w\n",
    "            for feature, value in x.items():\n",
    "                if(y * (dotpro + self.theta) > 1):\n",
    "                    dldw = 0                   \n",
    "                else:\n",
    "                    dldw = -y * value\n",
    "                self.G[feature] += dldw ** 2\n",
    "                self.w[feature] += self.eta * y * value / np.sqrt(self.G[feature])\n",
    "                \n",
    "#             update theta        \n",
    "            if(y * (dotpro + self.theta) > 1):\n",
    "                dldtheta = 0\n",
    "            else:\n",
    "                dldtheta = -y\n",
    "            self.H += dldtheta ** 2\n",
    "            self.theta += self.eta * y / np.sqrt(self.H)  \n",
    "            \n",
    "            self.lastM = self.M\n",
    "            \n",
    "        else:\n",
    "            self.M += 1\n",
    "        \n",
    "\n",
    "    def predict_single(self, x):\n",
    "        \"\"\" TODO: IMPLEMENT\"\"\"\n",
    "\n",
    "        score = 0\n",
    "        for feature, value in x.items():\n",
    "            score += self.w[feature] * value\n",
    "        score += self.theta\n",
    "        \n",
    "        if score <= 0:\n",
    "            return -1\n",
    "        return 1\n",
    "        \n",
    "    def finalize(self):\n",
    "        \"\"\" TODO: IMPLEMENT\"\"\"\n",
    "        self.M += 1\n",
    "        for feature in self.weightedW:\n",
    "            self.w[feature] = 1/self.M * self.weightedW[feature]\n",
    "        self.theta = 1/self.M * self.weightedT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9445\n"
     ]
    }
   ],
   "source": [
    "# test the averaged Adagrad model\n",
    "\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test, features = load_synthetic_data('dense')\n",
    "\n",
    "# 𝜂∈{1.5,0.25,0.03,0.005,0.001}\n",
    "\n",
    "eta = 1.5\n",
    "AvgAdaGrad = AveragedAdaGrad(eta,features)\n",
    "AvgAdaGrad.train(X_train,y_train)\n",
    "# OrigAdaGrad = AdaGrad(eta,features)\n",
    "# OrigAdaGrad.train(X_train,y_train)\n",
    "\n",
    "count = 0\n",
    "for i in range(len(X_dev)):\n",
    "    x = X_dev[i]\n",
    "    y = AvgAdaGrad.predict_single(x)\n",
    "    if y_dev[i] == y:\n",
    "        count += 1\n",
    "print(count/len(X_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMi5gf710aRr"
   },
   "source": [
    "### 1.6 Support Vector Machines\n",
    "\n",
    "Although we have not yet covered SVMs in class, you can still train them using the `sklearn` library.\n",
    "We will use a soft margin SVM for non-linearly separable data.\n",
    "You should use the `sklearn` implementation as follows:\n",
    "```\n",
    "from sklearn.svm import LinearSVC\n",
    "classifier = LinearSVC(loss='hinge')\n",
    "classifier.fit(X, y)\n",
    "```\n",
    "\n",
    "`sklearn` requires a different feature representation than what we use for the Perceptron models.\n",
    "The provided Python template code demonstrates how to convert to the require representation.\n",
    "\n",
    "\n",
    "Given training samples $S = \\{(\\mathbf{x}^1, y^1), (\\mathbf{x}^2, y^2), \\dots, (\\mathbf{x}^m, y^m)\\}$, the objective for the SVM is the following:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\min_{\\mathbf{w}, b, \\boldsymbol{\\xi}} \\frac{1}{2} \\vert\\vert \\mathbf{w}\\vert\\vert^2_2 + C \\sum_{i=1}^m \\xi_i\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "subject to the following constraints:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    y^i(\\mathbf{w}^\\intercal \\mathbf{x}^i + b) \\geq 1 - \\xi_i \\;\\;&\\textrm{for } i = 1, 2, \\dots, m \\\\\n",
    "    \\xi_i \\geq 0 \\;\\;& \\textrm{for } i = 1, 2, \\dots, m\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier(Classifier):\n",
    "    \n",
    "    def __init__(self):\n",
    "        from sklearn.svm import LinearSVC\n",
    "        self.local_classifier = LinearSVC(loss = 'hinge')\n",
    "        self.vectorizer = DictVectorizer()\n",
    "        \n",
    "    def trainSVM(self,X_train,y_train):\n",
    "        X_train_dict = self.vectorizer.fit_transform(X_train)\n",
    "        self.local_classifier.fit(X_train_dict,y_train)\n",
    "    \n",
    "    def testSVM(self,X_test,y_test):\n",
    "        X_test_dict = self.vectorizer.transform(X_test)\n",
    "        return self.local_classifier.score(X_test_dict,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "ys83AbV50aRt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9405"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"TODO: Create an SVM classifier\"\"\"\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test, features = load_synthetic_data('dense')\n",
    "\n",
    "# This is how you convert from the way we represent features in the\n",
    "# Perceptron code to how you need to represent features for the SVM.\n",
    "# You can then train with (X_train_dict, y_train) and test with\n",
    "# (X_conll_test_dict, y_conll_test) and (X_enron_test_dict, y_enron_test)\n",
    "vectorizer = DictVectorizer()\n",
    "X_train_dict = vectorizer.fit_transform(X_train)\n",
    "X_test_dict = vectorizer.transform(X_test)\n",
    "\n",
    "    \n",
    "from sklearn.svm import LinearSVC\n",
    "classifier = LinearSVC(loss = 'hinge')\n",
    "classifier.fit(X_train_dict,y_train)\n",
    "classifier.score(X_test_dict,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzmeTfF20aR0"
   },
   "source": [
    "# Section 2. Datasets\n",
    "\n",
    "In this section, we describe the synthetic and NER datasets that you will use for your experiments.\n",
    "For the NER datasets, there is also an explanation for the features which you need to extract from the data.\n",
    "\n",
    "### 2.1 Synthetic Data\n",
    "\n",
    "#### 2.1.1 Introduction\n",
    "\n",
    "The synthetic datasets have features and labels which are automatically generated from a python script.\n",
    "Each instance will have $n$ binary features and are labeled according to a $l$-of-$m$-of-$n$ Boolean function.\n",
    "Specifically, there is a set of $m$ features such that an example if positive if and only if at least $l$ of these $m$ features are active.\n",
    "The set of $m$ features is the same for the dataset (i.e., it is not a separate set of $m$ features for each individual instance).\n",
    "\n",
    "We provide two versions of the synthetic dataset called sparse and dense.\n",
    "For both datasets, we set $l = 10$ and $m=20$.\n",
    "We set $n = 200$ for the sparse data and $n = 40$ for the dense data.\n",
    "Additionally, we add noise to the data as follows:\n",
    "With probability $0.05$ the label assigned by the function is changed and with probability $0.001$ each feature value is changed.\n",
    "Consequently, the data is not linearly separable.\n",
    "\n",
    "We have provided you with three data splits for both sparse and dense with 10,000 training, 2,000 development, and 2,000 testing examples.\n",
    "Section 3 describes the experiments that you need to run on these datasets.\n",
    "\n",
    "#### 2.1.2 Feature Representation\n",
    "\n",
    "The features of the synthetic data provided are vectors of 0s and 1s.\n",
    "Storing these large matrices requires lots of memory so we use a sparse representation that stores them as dictionaries instead.\n",
    "For example, the vector $[0,1,0,0,0,1]$ can be stored as `{\"x2\": 1,\"x6\": 1}` (using 1-based indexing).\n",
    "We have provided you with the code for parsing and converting the data to this format.\n",
    "You can use these for the all algorithms you develop except the SVM.\n",
    "Since you will be using the implementation of SVM from sklearn, you will need to provide a vector to it. You can use `sklearn.feature_extraction.DictVectorizer` for converting feature-value dictionaries to vectors.\n",
    "\n",
    "### 2.2 NER Data\n",
    "\n",
    "In addition to the synthetic data, we have provided you two datasets for the task of named-entity recognition (NER).\n",
    "The goal is to identify whether strings in text represent names of people, organizations, or locations.\n",
    "An example instance looks like the following:\n",
    "\n",
    "```\n",
    "    [PER Wolff] , currently a journalist in [LOC Argentina] , played with\n",
    "    [PER del Bosque] in the final years of the seventies in\n",
    "    [ORG Real Madrid] .\n",
    "```\n",
    "\n",
    "In this problem, we will simplify the task to identifying whether a string is named entity or not (that is, you don't have to say which type of entity it is).\n",
    "For each token in the input, we will use the tag $\\texttt{I}$ to denote that token is an entity and $\\texttt{O}$ otherwise.\n",
    "For example, the full tagging for the above instace is as follows:\n",
    "\n",
    "```\n",
    "    [I Wolff] [O ,] [O currently] [a] [O journalist] [O in] [I Argentina]\n",
    "    [O ,] [O played] [O with] [I del] [I Bosque] [O in] [O the] [O final]\n",
    "    [O years] [O of] [O the] [O seventies] [O in] [I Real] [I Madrid] .\n",
    "```\n",
    "\n",
    "Given a sentence $S = w_1, w_2, \\dots, w_n$, you need to predict the `I`, `O` tag for each word in the sentence.\n",
    "That is, you will produce the sequence $Y = y_1, y_2, \\dots, y_n$ where $y_i \\in$ {`I`, `O`}.\n",
    "\n",
    "#### 2.2.1 Datasets: CoNLL and Enron\n",
    "\n",
    "We have provided two datasets, the CoNLL dataset which is text from news articles, and Enron, a corpus of emails.\n",
    "The files contain one word and one tag per line.\n",
    "For CoNLL, there are training, development, and testing files, whereas Enron only has a test dataset.\n",
    "There are 14,987 training sentences (204,567 words), 336 development sentences (3,779 words), and 303 testing sentences (3,880 words) in CoNLL.\n",
    "For Enron there are 368 sentences (11,852 words).\n",
    "\n",
    "**Please note that the CoNLL dataset is available only for the purposes of this assignment.\n",
    "It is copyrighted, and you are granted access because you are a Penn student, but please delete it when you are done with the homework.**\n",
    "\n",
    "#### 2.2.2 Feature Extraction\n",
    "\n",
    "The NER data is provided as raw text, and you are required to extract features for the classifier.\n",
    "In this assignment, we will only consider binary features based on the context of the word that is supposed to be tagged.\n",
    "\n",
    "Assume that there are $V$ unique words in the dataset and each word has been assigned a unique ID which is a number $\\{1, 2, \\dots, V\\}$.\n",
    "Further, $w_{-k}$ and $w_{+k}$ indicate the $k$th word before and after the target word.\n",
    "The feature templates that you should use to generate features are as follows:\n",
    "\n",
    "| Template             | Number of Features |\n",
    "|----------------------|--------------------|\n",
    "| $w_{-3}$             | $V$                |\n",
    "| $w_{-2}$             | $V$                |\n",
    "| $w_{-1}$             | $V$                |\n",
    "| $w_{+1}$             | $V$                |\n",
    "| $w_{+2}$             | $V$                |\n",
    "| $w_{+3}$             | $V$                |\n",
    "| $w_{-1}$ & $w_{-2}$  | $V \\times V$       |\n",
    "| $w_{+1}$ \\& $w_{+2}$ | $V \\times V$       |\n",
    "| $w_{-1}$ \\& $w_{+1}$ | $V \\times V$       |\n",
    "\n",
    "Each feature template corresponds to a set of features that you will compute (similar to the features you generated in problem 2 from the first homework assignment).\n",
    "The $w_{-3}$ feature template corresponds to $V$ features where the $i$th feature is 1 if the third word to the left of the target word has ID $i$.\n",
    "The $w_{-1} \\& w_{+1}$ feature template corresponds to $V \\times V$ features where there is one feature for every unique pair words.\n",
    "For example, feature $(i - 1) \\times V + j$ is a binary feature that is 1 if the word 1 to the left of the target has ID $i$ and the first word to the right of the target has ID $j$.\n",
    "In practice, you will not need to keep track of the feature IDs.\n",
    "Instead, each feature will be given a name such as \"$w_{-1}=\\textrm{the} \\& w_{+1}=\\textrm{cat}$\".\n",
    "\n",
    "In total, all of the above feature templates correspond to a very large number of features.\n",
    "However, for each word, there will be exactly 9 features which are active (non-zero), so the feature vector is quite sparse.\n",
    "You will represent this as a dictionary which maps from the feature name to the value.\n",
    "In the provided Python template, we have implemented a couple of the features for you to demonstrate how to compute them and what the naming scheme should look like.\n",
    "\n",
    "In order to deal with the first two words and the last two words in a sentence, we will add special symbol \"SSS\" and \"EEE\" to the vocabulary to represent the words before the first word and the words after the last word.\n",
    "Notice that in the test data you may encounter a word that was not observed in training, and therefore is not in your dictionary.\n",
    "In this case, you cannot generate a feature for it, resulting in less than 7 active features in some of the test examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHHy_nGx0aR2"
   },
   "source": [
    "# Section 3. Experiments (65 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PKCZQ9Z0aR3"
   },
   "source": [
    "You will run two sets of experiments, one using the synthetic data and one using the NER data.\n",
    "\n",
    "### 3.1 Synthetic Experiment (30 + 10 extra credit points)\n",
    "\n",
    "This experiment will explore the impact that the amount of training data has on model performance.\n",
    "First, you will do hyperparameter tuning for Winnow and Perceptron with AdaGrad (both standard and averaged versions).\n",
    "Then you will generate learning curves that will plot the size of the training data against the performance.\n",
    "Finally, for each of the models trained on all of the training data, you will find the test score.\n",
    "You should use accuracy to compute the performance of the model.\n",
    "\n",
    "In summary, the experiment consists of three parts\n",
    "1. Parameter Tuning\n",
    "2. Learning Curves\n",
    "3. Final Evaluation\n",
    "\n",
    "#### 3.1.1 Parameter Tuning (10 points)\n",
    "\n",
    "For both the Winnow and Perceptron with AdaGrad (standard and averaged), there are hyperparameters that you need to choose.\n",
    "(The same is true for SVM, but you should only use the default settings.)\n",
    "Similarly to cross-validation from Homework 1, we will estimate how well each model will do on the true test data using the development dataset (we will not run cross-validation), and choose the hyperparameter settings based on these results.\n",
    "\n",
    "For each hyperparameter value in Section 1, train a model using that value on the training data and compute the accuracy on the development dataset. Each model should be trained for 10 iterations (i.e., 10 passes over the entire dataset).\n",
    "\n",
    "TODO: Fill in the table with the best hyperparameter values and the corresponding validation accuracies.\n",
    "Repeat this for both the sparse and dense data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdoFHNKp0aR4"
   },
   "source": [
    "# Winnow Sweep\n",
    "\n",
    "| $\\alpha$ | Sparse | Dense |\n",
    "|----------|--------|-------|\n",
    "| 1.1      | 0.8935 | 0.8995 |\n",
    "| 1.01     | 0.9270    | 0.9215     |\n",
    "| 1.005    | 0.9195    | 0.9080   |\n",
    "| 1.0005   | 0.5630     | 0.8615     |\n",
    "| 1.0001   | 0.5205      | 0.6140      |\n",
    "\n",
    "##### Averaged Winnow Sweep\n",
    "\n",
    "| $\\alpha$ | Sparse | Dense |\n",
    "|----------|--------|-------|\n",
    "| 1.1      |  0.9390      | 0.9445      |\n",
    "| 1.01     |  0.8980      | 0.9335      |\n",
    "| 1.005    |  0.8405      | 0.9150      |\n",
    "| 1.0005   |  0.5255      | 0.6700      |\n",
    "| 1.0001   |  0.5095      | 0.5460      |\n",
    "\n",
    "##### AdaGrad Sweep\n",
    "\n",
    "| $\\eta$   | Sparse | Dense |\n",
    "|----------|--------|-------|\n",
    "| 1.5      |  0.8745      | 0.9325      |\n",
    "| 0.25     |  0.8745      | 0.9325      |\n",
    "| 0.03     |  0.8745      | 0.9325      |\n",
    "| 0.005    |  0.8745      | 0.9325      |\n",
    "| 0.001    |  0.8745      | 0.9325      |\n",
    "\n",
    "##### Averaged AdaGrad Sweep\n",
    "\n",
    "| $\\eta$   | Sparse | Dense |\n",
    "|----------|--------|-------|\n",
    "| 1.5      | 0.8935       | 0.9445      |\n",
    "| 0.25     | 0.8935       | 0.9445      |\n",
    "| 0.03     | 0.8935       | 0.9445      |\n",
    "| 0.005    | 0.8935       | 0.9445      |\n",
    "| 0.001    | 0.8935       | 0.9445      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHZZ6U240aR6"
   },
   "source": [
    "#### 3.1.2 Learning Curves (10 points)\n",
    "\n",
    "Next, you will train all 7 models with different amounts of training data.\n",
    "For Winnow and Perceptron with AdaGrad (standard and averaged), use the best hyperparameters from the parameter tuning experiment.\n",
    "\n",
    "Each of the datasets contains 10,000 training examples.\n",
    "You will train each model 11 times on varying amounts of training data.\n",
    "The first 10 will increase by 500 examples: 500, 1k, 1.5k, 2k, ..., 5k.\n",
    "The 11th model should use all 10k examples.\n",
    "Each Perceptron-based model should be trained for 10 iterations (e.g., 10 passes over the total number of training examples available to that model).\n",
    "The SVM can be run until convergence with the default parameters.\n",
    "\n",
    "For each model, compute the accuracy on the development dataset and plot the results using the provided code.\n",
    "There should be a separate plot for the sparse and dense data.\n",
    "\n",
    "**Note** how we have included an image in markdown. You should do the same for both plots and include them in the output below by running your experiment, saving your plots to the images folder, and linking it to this cell.\n",
    "\n",
    "##### Sparse Plot \n",
    "\n",
    "![sparse](images/part313_sparse1.png)\n",
    "\n",
    "##### Dense Plot\n",
    "\n",
    "![part313_dense](images/part313_dense1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the learning curve variables and load the dataset\n",
    "\n",
    "perceptron_accs = np.zeros(11)\n",
    "winnow_accs = np.zeros(11)\n",
    "adagrad_accs = np.zeros(11)\n",
    "avg_perceptron_accs = np.zeros(11)\n",
    "avg_winnow_accs = np.zeros(11)\n",
    "avg_adagrad_accs = np.zeros(11)\n",
    "svm_accs = np.zeros(11)\n",
    "\n",
    "\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test, features = load_synthetic_data('sparse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic perceptron\n",
    "for i in range(1,12):\n",
    "    X_train_i = X_train[0:(i*500)]\n",
    "    y_train_i = y_train[0:(i*500)]\n",
    "    if i == 11:\n",
    "        X_train_i = X_train\n",
    "        y_train_i = y_train\n",
    "    eta = 1.5\n",
    "    model = Perceptron(features)\n",
    "    model.train(X_train_i,y_train_i)\n",
    "    perceptron_accs[i-1] = compute_accuracy_313(X_dev,y_dev,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaged perceptron\n",
    "for i in range(1,12):\n",
    "    X_train_i = X_train[0:(i*500)]\n",
    "    y_train_i = y_train[0:(i*500)]\n",
    "    if i == 11:\n",
    "        X_train_i = X_train\n",
    "        y_train_i = y_train\n",
    "    model = AveragedPerceptron(features)\n",
    "    model.train(X_train_i,y_train_i)\n",
    "    avg_perceptron_accs[i-1] = compute_accuracy_313(X_dev,y_dev,model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# winnow\n",
    "for i in range(1,12):\n",
    "    X_train_i = X_train[0:(i*500)]\n",
    "    y_train_i = y_train[0:(i*500)]\n",
    "    if i == 11:\n",
    "        X_train_i = X_train\n",
    "        y_train_i = y_train\n",
    "    alpha = 1.01\n",
    "    model = Winnow(alpha,features)\n",
    "    model.train(X_train_i,y_train_i)\n",
    "    winnow_accs[i-1] = compute_accuracy_313(X_dev,y_dev,model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaged winnow\n",
    "for i in range(1,12):\n",
    "    X_train_i = X_train[0:(i*500)]\n",
    "    y_train_i = y_train[0:(i*500)]\n",
    "    if i == 11:\n",
    "        X_train_i = X_train\n",
    "        y_train_i = y_train\n",
    "    alpha = 1.1\n",
    "    model = AveragedWinnow(alpha,features)\n",
    "    model.train(X_train_i,y_train_i)\n",
    "    avg_winnow_accs[i-1] = compute_accuracy_313(X_dev,y_dev,model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adagrad\n",
    "for i in range(1,12):\n",
    "    X_train_i = X_train[0:(i*500)]\n",
    "    y_train_i = y_train[0:(i*500)]\n",
    "    if i == 11:\n",
    "        X_train_i = X_train\n",
    "        y_train_i = y_train\n",
    "    eta = 1.5\n",
    "    model = AdaGrad(eta,features)\n",
    "    model.train(X_train_i,y_train_i)\n",
    "    adagrad_accs[i-1] = compute_accuracy_313(X_dev,y_dev,model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaged AdaGrad\n",
    "for i in range(1,12):\n",
    "    X_train_i = X_train[0:(i*500)]\n",
    "    y_train_i = y_train[0:(i*500)]\n",
    "    if i == 11:\n",
    "        X_train_i = X_train\n",
    "        y_train_i = y_train\n",
    "    eta = 1.5\n",
    "    model = AveragedAdaGrad(eta,features)\n",
    "    model.train(X_train_i,y_train_i)\n",
    "    avg_adagrad_accs[i-1] = compute_accuracy_313(X_dev,y_dev,model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tongd\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "for i in range(1,12):\n",
    "    X_train_i = X_train[0:(i*500)]\n",
    "    y_train_i = y_train[0:(i*500)]\n",
    "    if i == 11:\n",
    "        X_train_i = X_train\n",
    "        y_train_i = y_train\n",
    "    vectorizer = DictVectorizer()\n",
    "    X_train_dict = vectorizer.fit_transform(X_train_i)\n",
    "    X_dev_dict = vectorizer.transform(X_dev)\n",
    "    classifier = LinearSVC(loss = 'hinge')\n",
    "    classifier.fit(X_train_dict,y_train_i)\n",
    "    svm_accs[i-1] = classifier.score(X_dev_dict,y_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_learning_curves(perceptron_accs,\n",
    "                        winnow_accs,\n",
    "                        adagrad_accs,\n",
    "                        avg_perceptron_accs,\n",
    "                        avg_winnow_accs,\n",
    "                        avg_adagrad_accs,\n",
    "                        svm_accs\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.943 , 0.9445, 0.9445, 0.9445, 0.9445, 0.9445, 0.9445, 0.9445,\n",
       "       0.9445, 0.9445, 0.9445])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_313(X_dev,y_dev,classifier):\n",
    "    count = 0\n",
    "    for i in range(len(X_dev)):\n",
    "        x = X_dev[i]\n",
    "        y = classifier.predict_single(x)\n",
    "        if y_dev[i] == y:\n",
    "            count += 1\n",
    "    return (count/len(X_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wHcZsGo0aR8"
   },
   "source": [
    "#### 3.1.3 Final Evaluation (5 points)\n",
    "\n",
    "Finally, for each of the 7 models, train the models on all of the training data and compute the test accuracy.\n",
    "For Winnow and Perceptron with AdaGrad, use the best hyperparameter settings you found.\n",
    "Report these accuracies in a table.\n",
    "   \n",
    "We will run our models with [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 10000] examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCfmjoYw_wNQ"
   },
   "outputs": [],
   "source": [
    "sample_sizes = [500 * i for i in range(1, 11)] + [10_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "id": "ZJJ9u_kf0aSB"
   },
   "outputs": [],
   "source": [
    "def run_synthetic_experiment(dataset_type='sparse'):\n",
    "    \"\"\"\n",
    "    TODO: IMPLEMENT \n",
    "    \n",
    "    Runs the synthetic experiment on either the sparse or dense data\n",
    "    depending on the data path (e.g. \"data/sparse\" or \"data/dense\").\n",
    "    \n",
    "    We have provided how to train the Perceptron on the training and\n",
    "    test on the testing data (the last part of the experiment). You need\n",
    "    to implement the hyperparameter sweep, the learning curves, and\n",
    "    predicting on the test dataset for the other models.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test, features = load_synthetic_data(dataset_type)\n",
    "    \n",
    "\n",
    "    # TODO: YOUR CODE HERE. Determine the best hyperparameters for the relevant models\n",
    "    # report the validation accuracy after training using the best hyper parameters\n",
    "    \n",
    "#     i think i did all of these picked the best hyper parameters way above here near the hyper parameter tables\n",
    "        \n",
    "    \n",
    "    # TOOD: YOUR CODE HERE. Downsample the dataset to the number of desired training\n",
    "    # instances (e.g. 500, 1000), then train all of the models on the\n",
    "    # sampled dataset. Compute the accuracy and add the accuracies to\n",
    "    # the corresponding list. Use plot_learning_curves()\n",
    "    \n",
    "#     I did all of these near where the plot is, separated into 7 or 8 cells. please refer to that.\n",
    "        \n",
    "    \n",
    "    # TODO: Train all 7 models on the training data and make predictions \n",
    "    # for test data\n",
    "    # We will show you how to do it for the basic Perceptron model.\n",
    "    classifier = Perceptron(features)\n",
    "    classifier.train(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Perceptron's accuracy is {acc}\")\n",
    "\n",
    "    # YOUR CODE HERE: Repeat for the other 6 models.\n",
    "\n",
    "#     Averaged Perceptron\n",
    "    classifier = AveragedPerceptron(features)\n",
    "    classifier.train(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Averaged Perceptron's accuracy is {acc}\")\n",
    "    \n",
    "#     Winnow\n",
    "    alpha = 1.01\n",
    "    classifier = Winnow(alpha,features)\n",
    "    classifier.train(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Winnow's accuracy is {acc}\")\n",
    "    \n",
    "#     Averaged Winnow\n",
    "    alpha = 1.1\n",
    "    classifier = AveragedWinnow(alpha,features)\n",
    "    classifier.train(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Averged Winnow's accuracy is {acc}\")\n",
    "    \n",
    "#     Averaged Winnow\n",
    "    eta = 1.5\n",
    "    classifier = AdaGrad(eta,features)\n",
    "    classifier.train(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"AdaGrad's accuracy is {acc}\")\n",
    "    \n",
    "#     Averaged Winnow\n",
    "    eta = 1.5\n",
    "    classifier = AveragedAdaGrad(eta,features)\n",
    "    classifier.train(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Averged AdaGrad's accuracy is {acc}\")\n",
    "    \n",
    "    vectorizer = DictVectorizer()\n",
    "    X_train_dict = vectorizer.fit_transform(X_train)\n",
    "    X_test_dict = vectorizer.transform(X_test)\n",
    "    classifier = LinearSVC(loss = 'hinge')\n",
    "    classifier.fit(X_train_dict,y_train)\n",
    "    acc = classifier.score(X_test_dict,y_test)\n",
    "    print(f\"SVM's accuracy is {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "Sz_oTWqC0aSE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron's accuracy is 0.717\n",
      "Averaged Perceptron's accuracy is 0.9135\n",
      "Winnow's accuracy is 0.926\n",
      "Averged Winnow's accuracy is 0.936\n",
      "AdaGrad's accuracy is 0.878\n",
      "Averged AdaGrad's accuracy is 0.884\n",
      "SVM's accuracy is 0.936\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run the synthetic experiment on the sparse dataset. For reference,\n",
    "\"synthetic/sparse\" is the path to where the data is located.\n",
    "Note: This experiment takes substantial time (around 15 minutes),\n",
    "so don't worry if it's taking a long time to finish.\n",
    "\"\"\"\n",
    "run_synthetic_experiment('sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "id": "lXcorX8ndhRo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron's accuracy is 0.9205\n",
      "Averaged Perceptron's accuracy is 0.9405\n",
      "Winnow's accuracy is 0.9255\n",
      "Averged Winnow's accuracy is 0.9405\n",
      "AdaGrad's accuracy is 0.9325\n",
      "Averged AdaGrad's accuracy is 0.9405\n",
      "SVM's accuracy is 0.9405\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run the synthetic experiment on the dense dataset. For reference,\n",
    "\"synthetic/dense\" is the path to where the data is located.\n",
    "Note: this experiment should take much less time.\n",
    "\"\"\"\n",
    "run_synthetic_experiment('dense')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2hLx_9Q0aSJ"
   },
   "source": [
    "##### Questions (5 points)\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. Discuss the trends that you see when comparing the standard version of an algorithm to the averaged version (e.g., Winnow versus Averaged Winnow). Is there an observable trend?\n",
    "    \n",
    "    Typically averaged version of the algorithm has a slightly higher accuracy than the standard version, only 0.01~0.02 improvement but quite consistent.\n",
    "\n",
    "\n",
    "2. We provided you with 10,000 training examples.\n",
    "   Were all 10,000 necessary to achieve the best performance for each classifier?\n",
    "   If not, how many were necessary? (Rough estimates, no exact numbers required)\n",
    "   \n",
    "   Not all were necessary. Looking at the learning rate graph, accuracy for both dense and sparse datasets plateau around 6000-7000 training examples for even the worst-performing algorithm. For the best-performing algorithm, SVM, it converge to the highest accuracy with only about 1000 training examples. Further examples did not provide any improvement over accuracy.\n",
    "\n",
    "\n",
    "3. Report your Final Test Accuracies\n",
    "\n",
    "   \n",
    "\n",
    "| Model               | Sparse | Dense |\n",
    "|---------------------|--------|-------|\n",
    "| Perceptron          |  0.7170      |   0.9205    |\n",
    "| Winnow              |  0.9260      |  0.9255     |\n",
    "| AdaGrad             |  0.8780      |  0.9325     |\n",
    "| Averaged Perceptron |  0.9135      |  0.9405     |\n",
    "| Averaged Winnow     |  0.9360      |  0.9405     |\n",
    "| Averaged AdaGrad    |  0.8840      |  0.9405     |\n",
    "| SVM                 |  0.9360      |  0.9405     |\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPZtn5Fv0aSK"
   },
   "source": [
    "#### 3.1.5 Extra Credit (10 points)\n",
    "\n",
    "Included in the resources for this homework assignment is the code that we used to generate the synthetic data.\n",
    "We used a small amount of noise to create the dataset which you ran the experiments on.\n",
    "For extra credit, vary the amount of noise in either/both of the label and features.\n",
    "Then, plot the models' performances as a function of the amount of noise.\n",
    "Discuss your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iW_koNXv79l"
   },
   "source": [
    "TODO: Extra Credit observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_dev, y_dev, X_test, y_test, features = load_synthetic_data('dense')\n",
    "noise = np.linspace(0,1,21)\n",
    "ec_winnowacc = np.zeros(len(noise))\n",
    "ec_perceptronacc = np.zeros(len(noise))\n",
    "ec_adagradacc = np.zeros(len(noise))\n",
    "ec_svmacc = np.zeros(len(noise))\n",
    "\n",
    "for i in range(len(noise)):\n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test, features = load_synthetic_data('dense')\n",
    "    classifier = Perceptron(features)\n",
    "    num = int(len(y_train) * noise[i])\n",
    "    for j in range(num):\n",
    "        tempRand = np.random.randint(0,len(y_train))\n",
    "        y_train[tempRand] = -y_train[tempRand]\n",
    "    classifier.train(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    ec_perceptronacc[i] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "for i in range(len(noise)):\n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test, features = load_synthetic_data('dense')\n",
    "    classifier = Winnow(1.1,features)\n",
    "    num = int(len(y_train) * noise[i])\n",
    "    for j in range(num):\n",
    "        tempRand = np.random.randint(0,len(y_train))\n",
    "        y_train[tempRand] = -y_train[tempRand]\n",
    "    classifier.train(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    ec_winnowacc[i] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "for i in range(len(noise)):\n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test, features = load_synthetic_data('dense')\n",
    "    classifier = AdaGrad(1.5,features)\n",
    "    num = int(len(y_train) * noise[i])\n",
    "    for j in range(num):\n",
    "        tempRand = np.random.randint(0,len(y_train))\n",
    "        y_train[tempRand] = -y_train[tempRand]\n",
    "    classifier.train(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    ec_adagradacc[i] = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "for i in range(len(noise)):\n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test, features = load_synthetic_data('dense')\n",
    "    num = int(len(y_train) * noise[i])\n",
    "    for j in range(num):\n",
    "        tempRand = np.random.randint(0,len(y_train))\n",
    "        y_train[tempRand] = -y_train[tempRand]\n",
    "    vectorizer = DictVectorizer()\n",
    "    X_train_dict = vectorizer.fit_transform(X_train)\n",
    "    X_test_dict = vectorizer.transform(X_test)\n",
    "    classifier = LinearSVC(loss = 'hinge')\n",
    "    classifier.fit(X_train_dict,y_train)\n",
    "    ec_svmacc[i] = classifier.score(X_test_dict,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-264-a5f7981b1d45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_EC_curves\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mec_perceptronacc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_winnowacc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_adagradacc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_svmacc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-263-ccb9b3293472>\u001b[0m in \u001b[0;36mplot_EC_curves\u001b[1;34m(perceptron_accs, winnow_accs, adagrad_accs, svm_accs)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_list\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m21\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;31m#         ax2.plot(x, acc_list, label=label)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ax' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_EC_curves(ec_perceptronacc,ec_winnowacc,ec_adagradacc,ec_svmacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A helper function that plots the relationship between number of examples\n",
    "and accuracies for all the models.\n",
    "\n",
    "You should not need to edit this method.\n",
    "\"\"\"\n",
    "def plot_EC_curves(\n",
    "    perceptron_accs,\n",
    "    winnow_accs,\n",
    "    adagrad_accs,\n",
    "    svm_accs\n",
    "):\n",
    "    \n",
    "    accuracies = [\n",
    "        ('perceptron', perceptron_accs),\n",
    "        ('winnow', winnow_accs),\n",
    "        ('adagrad', adagrad_accs),\n",
    "        ('svm', svm_accs)\n",
    "    ]\n",
    "    \n",
    "    x = np.linspace(0,1,21)\n",
    "    plt.figure()\n",
    "#     f, (ax, ax2) = plt.subplots(1, 2, sharey=True, facecolor='w')\n",
    "    \n",
    "    for label, acc_list in accuracies:\n",
    "        assert len(acc_list) == 21\n",
    "        ax.plot(x, acc_list, label=label)\n",
    "#         ax2.plot(x, acc_list, label=label)\n",
    "        \n",
    "    ax.set_xlim(0, 1)\n",
    "#     ax2.set_xlim(9500, 10000)\n",
    "#     ax2.set_xticks([10000])\n",
    "    # hide the spines between ax and ax2\n",
    "    ax.spines['right'].set_visible(False)\n",
    "#     ax2.spines['left'].set_visible(False)\n",
    "    ax.yaxis.tick_left()\n",
    "    ax.tick_params(labelright='off')\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.814 , 0.802 , 0.718 , 0.784 , 0.6285, 0.7525, 0.769 , 0.6545,\n",
       "       0.5415, 0.6905, 0.707 , 0.6945, 0.688 , 0.718 , 0.656 , 0.7205,\n",
       "       0.725 , 0.6525, 0.77  , 0.7515, 0.812 ])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec_winnowacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMisGYXj0aSK"
   },
   "source": [
    "### 3.2 NER Experiment: Welcome to the Real World (35 points)\n",
    "\n",
    "The experiment with the NER data will analyze how changing the domain of the training and testing data can impact the performance of a model.\n",
    "\n",
    "Instead of accuracy, you will use your $F_1$ score implementation in Section 0 to evaluate how well a model does.\n",
    "Recall measures how many of the actual entities the model successfully tagged as an entity.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\textrm{Precision} &= \\frac{\\#\\textrm{(Actually Entity & Model Predicted Entity)}}{\\#\\textrm{(Model Predicted Entity)}} \\\\\n",
    "    \\textrm{Recall} &= \\frac{\\#\\textrm{(Actually Entity & Model Predicted Entity)}}{\\#\\textrm{(Actually Entity)}} \\\\\n",
    "    \\textrm{F}_1 &= 2 \\cdot \\frac{\\textrm{Precision} \\times \\textrm{Recall}}{\\textrm{Precision} + \\textrm{Recall}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For this experiment, you will only use the averaged basic Perceptron and SVM.\n",
    "Hence, no parameter tuning is necessary.\n",
    "Train both models on the CoNLL training data then compute the F$_1$ on the development and testing data of both CoNLL and Enron.\n",
    "Note that the model which is used to predict labels for Enron is trained on CoNLL data, not Enron data.\n",
    "Report the F$_1$ scores in a table.\n",
    "\n",
    "#### 3.2.1 Extracting NER Features  (25 points)\n",
    "\n",
    "Reread Section 2.2.2 to understand how to extract the features required to train the models\n",
    "and translate it to the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "id": "Y0gPdPmp0aSL"
   },
   "outputs": [],
   "source": [
    "def extract_ner_features_train(train):\n",
    "    \"\"\"\n",
    "    Extracts feature dictionaries and labels from the data in \"train\"\n",
    "    Additionally creates a list of all of the features which were created.\n",
    "    We have implemented the w-1 and w+1 features for you to show you how\n",
    "    to create them.\n",
    "    \n",
    "    TODO: You should add your additional featurization code here.\n",
    "    (which might require adding and/or changing existing code)\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    X = []\n",
    "    features = set()\n",
    "    for sentence in train:\n",
    "        padded = [('SSS', None)] + [('SSS', None)] + [('SSS', None)] + sentence[:]\\\n",
    "                    + [('EEE', None)] + [('EEE', None)] + [('EEE', None)]\n",
    "        for i in range(3, len(padded) - 3):\n",
    "            y.append(1 if padded[i][1] == 'I' else -1)\n",
    "            feat1 = 'w-1=' + str(padded[i - 1][0])\n",
    "            feat2 = 'w+1=' + str(padded[i + 1][0])\n",
    "            feat3 = 'w-3=' + str(padded[i - 3][0])\n",
    "            feat4 = 'w-2=' + str(padded[i - 2][0])\n",
    "            feat5 = 'w+2=' + str(padded[i + 2][0])\n",
    "            feat6 = 'w+3=' + str(padded[i + 3][0])\n",
    "            feat7 = 'w-1&w-2=' + str(padded[i - 1][0] + str(padded[i - 2][0]))\n",
    "            feat8 = 'w+1&w+2=' + str(padded[i + 1][0] + str(padded[i + 2][0]))\n",
    "            feat9 = 'w-1&w+1=' + str(padded[i - 1][0] + str(padded[i + 1][0]))\n",
    "            feats = [feat1, feat2, feat3, feat4, feat5, feat6, feat7, feat8, feat9]\n",
    "            features.update(feats)\n",
    "            feats = {feature: 1 for feature in feats}\n",
    "            X.append(feats)\n",
    "    return features, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQOz2fZ_0aSV"
   },
   "source": [
    "Now, repeat the process of extracting features from the test data.\n",
    "What is the difference between the code above and below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "id": "TzqdP7oE0aSX"
   },
   "outputs": [],
   "source": [
    "def extract_features_dev_or_test(data, features):\n",
    "    \"\"\"\n",
    "    Extracts feature dictionaries and labels from \"data\". The only\n",
    "    features which should be computed are those in \"features\". You\n",
    "    should add your additional featurization code here.\n",
    "    \n",
    "    TODO: You should add your additional featurization code here.\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    X = []\n",
    "    for sentence in data:\n",
    "        padded = [('SSS', None)] + [('SSS', None)] + [('SSS', None)] + sentence[:]\\\n",
    "                    + [('EEE', None)] + [('EEE', None)] + [('EEE', None)]\n",
    "        for i in range(3, len(padded) - 3):\n",
    "            y.append(1 if padded[i][1] == 'I' else -1)\n",
    "            feat1 = 'w-1=' + str(padded[i - 1][0])\n",
    "            feat2 = 'w+1=' + str(padded[i + 1][0])\n",
    "            feat3 = 'w-3=' + str(padded[i - 3][0])\n",
    "            feat4 = 'w-2=' + str(padded[i - 2][0])\n",
    "            feat5 = 'w+2=' + str(padded[i + 2][0])\n",
    "            feat6 = 'w+3=' + str(padded[i + 3][0])\n",
    "            feat7 = 'w-1&w-2=' + str(padded[i - 1][0] + str(padded[i - 2][0]))\n",
    "            feat8 = 'w+1&w+2=' + str(padded[i + 1][0] + str(padded[i + 2][0]))\n",
    "            feat9 = 'w-1&w+1=' + str(padded[i - 1][0] + str(padded[i + 1][0]))\n",
    "            feats = [feat1, feat2, feat3, feat4, feat5, feat6, feat7, feat8, feat9]\n",
    "#             feats = [feat1, feat2]\n",
    "            feats = {feature: 1 for feature in feats if feature in features}\n",
    "            X.append(feats)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um-IG2cv0aSe"
   },
   "source": [
    "#### 3.2.2 Running the NER Experiment\n",
    "\n",
    "As stated previously, train both models on the CoNLL training data then compute the $F_1$  on the development and testing data of both CoNLL and Enron. Note that the model which is used to predict labels for Enron is trained on CoNLL data, not Enron data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "id": "jxvboxmg0aSe"
   },
   "outputs": [],
   "source": [
    "def run_ner_experiment(data_path):\n",
    "    \"\"\"\n",
    "    Runs the NER experiment using the path to the ner data\n",
    "    (e.g. \"ner\" from the released resources). We have implemented\n",
    "    the standard Perceptron below. You should do the same for\n",
    "    the averaged version and the SVM.\n",
    "    \n",
    "    The SVM requires transforming the features into a different\n",
    "    format. See the end of this function for how to do that.\n",
    "    \"\"\"\n",
    "    train = load_ner_data(dataset='conll', dataset_type='train')\n",
    "    conll_test = load_ner_data(dataset='conll', dataset_type='test')\n",
    "    enron_test = load_ner_data(dataset='enron', dataset_type='test')\n",
    "\n",
    "    features, X_train, y_train = extract_ner_features_train(train)\n",
    "    X_conll_test, y_conll_test = extract_features_dev_or_test(conll_test, features)\n",
    "    X_enron_test, y_enron_test = extract_features_dev_or_test(enron_test, features)\n",
    "                 \n",
    "    # TODO: We show you how to do this for Perceptron.\n",
    "    # You should do this for the Averaged Perceptron and SVM\n",
    "    classifier = Perceptron(features)\n",
    "    classifier.train(X_train, y_train)\n",
    "    \n",
    "    y_pred = classifier.predict(X_conll_test)\n",
    "    conll_f1 = calculate_f1(y_conll_test, y_pred)\n",
    "\n",
    "    y_pred = classifier.predict(X_enron_test)\n",
    "    enron_f1 = calculate_f1(y_enron_test, y_pred)\n",
    "    print('Perceptron on NER')\n",
    "    print('  CoNLL', conll_f1)\n",
    "    print('  Enron', enron_f1)\n",
    "    \n",
    "    \n",
    "#     Averaged Perceptron\n",
    "#     classifier = AveragedPerceptron(features)\n",
    "#     classifier.train(X_train, y_train)\n",
    "    \n",
    "#     y_pred = classifier.predict(X_conll_test)\n",
    "#     conll_f1 = calculate_f1(y_conll_test, y_pred)\n",
    "\n",
    "#     y_pred = classifier.predict(X_enron_test)\n",
    "#     enron_f1 = calculate_f1(y_enron_test, y_pred)\n",
    "#     print('Averaged Perceptron on NER')\n",
    "#     print('  CoNLL', conll_f1)\n",
    "#     print('  Enron', enron_f1)\n",
    "    \n",
    "    \n",
    "#     SVM\n",
    "    # This is how you convert from the way we represent features in the\n",
    "    # Perceptron code to how you need to represent features for the SVM.\n",
    "    # You can then train with (X_train_dict, y_train) and test with\n",
    "    # (X_conll_test_dict, y_conll_test) and (X_enron_test_dict, y_enron_test)\n",
    "    vectorizer = DictVectorizer()\n",
    "    X_train_dict = vectorizer.fit_transform(X_train)\n",
    "    X_conll_test_dict = vectorizer.transform(X_conll_test)\n",
    "    X_enron_test_dict = vectorizer.transform(X_enron_test)\n",
    "    classifier = LinearSVC(loss = 'hinge')\n",
    "    classifier.fit(X_train_dict,y_train)\n",
    "    \n",
    "    y_pred = classifier.predict(X_conll_test_dict)\n",
    "    conll_f1 = calculate_f1(y_conll_test,y_pred)\n",
    "    \n",
    "    y_pred = classifier.predict(X_enron_test_dict)\n",
    "    enron_f1 = calculate_f1(y_enron_test, y_pred)\n",
    "    print('Averaged Perceptron on NER')\n",
    "    print('  CoNLL', conll_f1)\n",
    "    print('  Enron', enron_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "id": "-yy6Y9320aSi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron on NER\n",
      "  CoNLL 0.7443890274314214\n",
      "  Enron 0.20623229461756373\n",
      "Averaged Perceptron on NER\n",
      "  CoNLL 0.827823691460055\n",
      "  Enron 0.24119530416221988\n"
     ]
    }
   ],
   "source": [
    "# Run the NER experiment. \"ner\" is the path to where the data is located.\n",
    "run_ner_experiment('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeElQnS_0aSw"
   },
   "source": [
    "\n",
    "##### F1 Scores Table \n",
    "TODO: report your values:  \n",
    "\n",
    "| Model               | CoNLL Test F1 | Enron Test F1 |\n",
    "|---------------------|---------------|---------------|\n",
    "| Averaged Perceptron |               |               |\n",
    "| SVM                 |   0.8278            |    0.2412           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpzm2yA20aSx"
   },
   "source": [
    "##### Questions (5 points)\n",
    "\n",
    "Comment on the results:\n",
    "1. Are the F$_1$ scores on CoNLL and Enron similar?\n",
    "    \n",
    "    No, F$_1$ scores on CoNLL and Enron are quite different. CoNLL has much higher ones than Enron.\n",
    "    \n",
    "\n",
    "2. If they are dissimilar, explain why you think the F$_1$ score increased/decreased on the Enron data.\n",
    "\n",
    "    CoNLL has both training and testing data so the classifiers are more fitted towards that type of data. Enron is a brand new group of examples that are unrelated to CoNLL so classifiers might have trouble identify it. This could be a sign of overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKr_PXhO0aSy"
   },
   "source": [
    "## Submission Instructions\n",
    "\n",
    "We will be using Gradescope to turn in both the Python code.\n",
    "You should have been automatically added to Gradescope.\n",
    "If you do not have access, please ask the TA staff on Piazza.\n",
    "\n",
    "There are three parts to the submission on Gradescope:\n",
    "* ipynb file: Submit this notebook (.ipynb). If you are using Google Colab, you will have to download it as an ipynb file.\n",
    "* PDF: This notebook saved as a PDF. We will use this to see the overall structure of your homework and code, and to check manual questions like the tables, plots, and your discussions. How should I convert this notebook to PDF, you may ask? There are a few ways, but for simplicity print the Jupyter notebook and _Save as PDF_.\n",
    "* Code: A `hw2.py` file. We will use this to unit test and automate grading some parts of the homework. We only need a few functions/classes from the whole notebook. In particular, to unit test we only need:\n",
    "    - `calculate_f1`\n",
    "    - `highest_and_lowest_f1_score`\n",
    "    - `Perceptron`, `Winnow`, and `AdaGrad` classes.\n",
    "    \n",
    "There are two ways to submit these pieces of code. You can either manually copy and paste these to a Python file,\n",
    "or you can use Jupyter's _Download as .py_, and delete all unnecessary code. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
